{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf26b8d-bbef-47c4-98b1-38224a3d3921",
   "metadata": {},
   "source": [
    "#### Quick Tour\n",
    "\n",
    "<!-- !pip install transformers datasets\n",
    "!pip install pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    " -->\n",
    "##### 1. Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640ab49-62cc-4137-b1bb-2223ca10cf3d",
   "metadata": {},
   "source": [
    "The [pipeline()](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline) downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. \n",
    "Now you can use the classifier on your target text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbd9ead-1d32-4a22-b57b-07d964776a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 14:17:12.557099: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-23 14:17:12.557128: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-23 14:17:12.557143: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-23 14:17:12.561001: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def sentiment_analysis():\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    print(classifier(\"We are very happy to show you the ðŸ¤— Transformers library.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7fb21-3e84-41b6-8ef3-f11f9fa02076",
   "metadata": {},
   "source": [
    "If you have more than one input, pass your inputs as a list to the [pipeline()](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline) to return a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd693713-e70c-4cab-9958-d54bcb8c6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_input_classifier():\n",
    "    results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"])\n",
    "    for result in results:\n",
    "        print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b42f80-dff9-4c88-b110-8340510e78dd",
   "metadata": {},
   "source": [
    "Load an audio dataset (see the ðŸ¤— Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart#audio) for more details) youâ€™d like to iterate over. For example, load the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset:\n",
    "\n",
    "and You need to make sure the sampling rate of the dataset matches the sampling rate [facebook/wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h) was trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9c482cf-f20e-4d06-aea9-7e79b0edc859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I FURN A JOINA COUT']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def speech_recognition():\n",
    "    speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
    "    from datasets import load_dataset, Audio\n",
    "    dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))\n",
    "    # The audio files are automatically loaded and resampled when calling the \"audio\" column. \n",
    "    # Extract the raw waveform arrays from the first 4 samples and pass it as a list to the pipeline:\n",
    "    result = speech_recognizer(dataset[:4][\"audio\"])\n",
    "    print([d[\"text\"] for d in result])\n",
    "    \n",
    "    # expected output:\n",
    "    # ['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \n",
    "    # \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\", \n",
    "    # \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", \n",
    "    # 'HOW DO I FURN A JOINA COUT']\n",
    " \n",
    "speech_recognition()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdc2b6-d507-47ae-8690-5dcc3284c39f",
   "metadata": {},
   "source": [
    "For larger datasets where the inputs are big (like in speech or vision), youâ€™ll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the [pipeline API reference](https://huggingface.co/docs/transformers/main_classes/pipelines) for more information.\n",
    "\n",
    "\n",
    "The **pipeline()** can accommodate any model from the [Hub](https://huggingface.co/models), making it easy to adapt the [pipeline()](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline) for other use-cases. For example, if youâ€™d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) finetuned for sentiment analysis you can use for French text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a30df-bc1e-44d3-ba72-cdd8cb875781",
   "metadata": {},
   "source": [
    "##### 2. Use another model and tokenizer in the pipeline\n",
    "The [pipeline()](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline) can accommodate any model from the [Hub](https://huggingface.co/models), making it easy to adapt the [pipeline()](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline) for other use-cases. For example, if youâ€™d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) finetuned for sentiment analysis you can use for French text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25215d5-cd80-492a-b3d7-e490ff6689d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58097d8-e264-4af0-9871-cb997cc5adf7",
   "metadata": {},
   "source": [
    "Use [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and [AutoTokenizer](https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/auto#transformers.AutoTokenizer) to load the pretrained model and itâ€™s associated tokenizer (more on an AutoClass in the next section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69311bf3-bfad-4ac5-9930-6402192b9560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5becb9c25cf461598b0ab1f20d3ade1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407c18a-a24e-4e7d-8362-266496784619",
   "metadata": {},
   "source": [
    "Specify the model and tokenizer in the [pipeline()](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline), and now you can apply the **classifier** on French text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c75753-31d7-4e94-8fae-bd3ea9822b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272652387619019}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.\")\n",
    "# epxected output: [{'label': '5 stars', 'score': 0.7273}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a115a94-bd9a-4b40-8b9f-d04a9891cc73",
   "metadata": {},
   "source": [
    "If you canâ€™t find a model for your use-case, youâ€™ll need to finetune a pretrained model on your data. Take a look at our [finetuning tutorial](https://huggingface.co/docs/transformers/training) to learn how. Finally, after youâ€™ve finetuned your pretrained model, please consider [sharing](https://huggingface.co/docs/transformers/model_sharing) the model with the community on the Hub to democratize machine learning for everyone! ðŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc0255-7877-4199-99b6-e5ffec524344",
   "metadata": {},
   "source": [
    "##### 3. AutoClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbc97d-98d7-4450-b7dc-a76398dd737e",
   "metadata": {},
   "source": [
    "[Video Tutorial - Instantiate a Transformer model pytorch](https://youtu.be/AhChOFRegn4)\n",
    "\n",
    "Under the hood, the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and [AutoTokenizer](https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/auto#transformers.AutoTokenizer) classes work together to power the [pipeline()](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline) you used above. An [AutoClass](https://huggingface.co/docs/transformers/model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate AutoClass for your task and itâ€™s associated preprocessing class.\n",
    "\n",
    "Letâ€™s return to the example from the previous section and see how you can use the AutoClass to replicate the results of the [pipeline()](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline).\n",
    "\n",
    "The AutoConfig API allows you to **instantiate** the configuration of a pretrianed model from any checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e8805ee-d94a-4299-9a78-d4ebeb035de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "def autoconfig_from_pretained():\n",
    "    bert_config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "    print(type(bert_config))\n",
    "    \n",
    "    gpt2_config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "    print(type(gpt2_config))\n",
    "    \n",
    "    bart_config = AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
    "    print(type(bart_config))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd506f93-999b-44b0-b14f-e31ab03a49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same architecture as bert-base-cased\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Note that these model only have same configuration (e.g. input size, net structure, output size)\n",
    "# but weights are randomly initialized if pretrained weights are not loaded\n",
    "def initialize_bert_config_model():\n",
    "    bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "    bert_model = BertModel(bert_config)\n",
    "\n",
    "    # specify num_hidden_layers parameter if you want use 10 layers instead of 12 layers in bert\n",
    "    bert_config = BertConfig.from_pretrained(\"bert-base-cased\", num_hidden_layers=10)\n",
    "    # here BertModel should be a 'callalbe' class\n",
    "    bert_model = BertModel(bert_config)\n",
    "\n",
    "# saving a model\n",
    "def save_a_model():\n",
    "    bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "    bert_model = BertModel(bert_config)\n",
    "    # some training code\n",
    "    bert_model.save_pretrained(\"my-bert-model\")\n",
    "\n",
    "# reloading a saved model\n",
    "def reload_a_saved_model():\n",
    "    bert_model = BertModel.from_pretrained(\"my-bert-model\")\n",
    "# this is what BertConfig look like\n",
    "# probably in json format\n",
    "\n",
    "    # BertConfig {\n",
    "    #     \"architectures\": [\n",
    "    #         \"BertForMaskedLM\"\n",
    "    #     ],\n",
    "    #     \"attention_probs_dropout_prob\": 0000,\n",
    "    #     \"gradient_checkpointing\": 0000,\n",
    "    #     \"hidden_act\": \"gelu\", \n",
    "    #     \"hidden_dropout_prob\": 0.1,\n",
    "    #     \"hidden_size\": 768, \n",
    "    #     \"initializer_range\": 0.02, \n",
    "    #     \"intermediate_size\": 3072, \n",
    "    #     \"layer_norm_eps\": 1e-12, \n",
    "    #     \"max_position_embeddings\": 512, \n",
    "    #     \"model_type\": \"bert\",\n",
    "    #     \"num_attention_heads\": 12, \n",
    "    #     \"num_hidden_layers\": 12, \n",
    "    #     \"pad_token_id\": 0, \n",
    "    #     \"position_embedding_type\": \"absolute\", \n",
    "    #     \"transformers_version\": \"4.6.0.dev0\",\n",
    "    #     \"type_vocab_size\": 2, \n",
    "    #     \"use_cache\": true, \n",
    "    #     \"vocab_size\": 28996\n",
    "    # }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf0082-d72b-4518-9498-c21a521037f3",
   "metadata": {},
   "source": [
    "##### 4. AutoTokenizer\n",
    "A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the [tokenizer summary](https://huggingface.co/docs/transformers/tokenizer_summary)). The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure youâ€™re using the same tokenization rules a model was pretrained with.\n",
    "\n",
    "Load a tokenizer with [AutoTokenizer](https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/auto#transformers.AutoTokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96aaab2e-6a76-4855-b5da-af5dfbad27e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Pass your text to the tokenizer:\n",
    "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(encoding)\n",
    "\n",
    "# expected output:\n",
    "# {'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],\n",
    "#  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416e7ba-7288-4192-aa22-9d143b065f38",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary containing:\n",
    "\n",
    " * [input_ids](https://huggingface.co/docs/transformers/glossary#input-ids): numerical representations of your tokens.\n",
    " * [attention_mask](https://huggingface.co/docs/transformers/.glossary#attention-mask): indicates which tokens should be attended to.\n",
    "\n",
    "A tokenizer can also accept a list of inputs, and pad and truncate the text to return a batch with uniform length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51f635f-ad8c-49f5-910c-57eace9f3880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_pt_batch():\n",
    "    pt_batch = tokenizer(\n",
    "        [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49918f8e-c512-4719-acd4-2d3c82f8aaee",
   "metadata": {},
   "source": [
    "Check out the [preprocess](https://huggingface.co/docs/transformers/preprocessing) tutorial for more details about tokenization, and how to use an [AutoImageProcessor](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoImageProcessor), [AutoFeatureExtractor](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoFeatureExtractor) and [AutoProcessor](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoProcessor) to preprocess image, audio, and multimodal inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2bddd-fb8b-4ce5-a055-883c60698b73",
   "metadata": {},
   "source": [
    "##### 5. AutoModel\n",
    "\n",
    "ðŸ¤— Transformers provides a simple and unified way to load pretrained instances. This means you can load an [AutoModel](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoModel) like you would load an [AutoTokenizer](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoTokenizer). The only difference is selecting the correct [AutoModel](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoModel) for the task. For text (or sequence) classification, you should load [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoModelForSequenceClassification):\n",
    "\n",
    "See the [task summary](https://huggingface.co/docs/transformers/task_summary) for tasks supported by an [AutoModel](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoModel) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c358f0-7d2f-4a88-a9a5-761354ba072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_model():\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    \n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    # Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding **:\n",
    "    \n",
    "    pt_outputs = pt_model(**pt_batch)\n",
    "    \n",
    "    # The model outputs the final activations in the *logits* attribute. \n",
    "    # Apply the softmax function to the *logits* to retrieve the probabilities:\n",
    "    \n",
    "    from torch import nn\n",
    "    \n",
    "    pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "    print(pt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d4d80-6686-4069-94ec-462f736fa0d0",
   "metadata": {},
   "source": [
    "All ðŸ¤— Transformers models (PyTorch or TensorFlow) output the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss. Model outputs are special dataclasses so their attributes are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary (you can index with an integer, a slice or a string) in which case, attributes that are None are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6bf2b-99bb-4f05-b451-a7c306557591",
   "metadata": {},
   "source": [
    "##### 6. Save a model\n",
    "\n",
    "Once your model is fine-tuned, you can save it with its tokenizer using [PreTrainedModel.save_pretrained()](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe8836-4d24-4b80-ae6a-35e36897e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_a_model():\n",
    "    pt_save_directory = \"./pt_save_pretrained\"\n",
    "    tokenizer.save_pretrained(pt_save_directory)\n",
    "    pt_model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e91982-22ce-4891-97c4-0aac8a89e3d2",
   "metadata": {},
   "source": [
    "When you are ready to use the model again, reload it with [PreTrainedModel.from_pretrained()](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e981cf-e28b-4b27-8357-6e72d4a4e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8124bb-e3de-4aab-b829-23190b50ca4a",
   "metadata": {},
   "source": [
    "One particularly cool ðŸ¤— Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The *from_pt* or *from_tf parameter* can convert the model from one framework to the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc241b-6157-4638-a79d-685bb1ba453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "def convert_tf_to_pt():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
    "    # the following line load a TensorFlow(tf) format model and convert to pytorch format model\n",
    "    # by specifying parameter 'from_tf=True'\n",
    "    pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c63ca7-2ee5-4b6c-88e4-ab7055bc8d19",
   "metadata": {},
   "source": [
    "##### 7. Custom model builds\n",
    "\n",
    "You can modify the modelâ€™s configuration class to change how a model is built. The configuration specifies a modelâ€™s attributes, such as the number of hidden layers or attention heads. You start from scratch when you initialize a model from a custom configuration class. The model attributes are randomly initialized, and youâ€™ll need to train the model before you can use it to get meaningful results.\n",
    "\n",
    "Start by importing [AutoConfig](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoConfig), and then load the pretrained model you want to modify. Within [AutoConfig.from_pretrained()](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.AutoConfig.from_pretrained), you can specify the attribute you want to change, such as the number of attention heads:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86476838-7b0a-4ae2-b23a-473afc96b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_Config():\n",
    "    from transformers import AutoConfig\n",
    "    my_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a5a96f-bc3d-492b-95fa-6d01179efd55",
   "metadata": {},
   "source": [
    "You can also create a model from your custom configuration with [AutoModel.from_config()](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a6618-be3e-435c-841b-20334f26ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "\n",
    "# my_model = AutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb810a7b-649a-405c-ac09-ce172cba70c7",
   "metadata": {},
   "source": [
    "#### Trainer - a PyTorch optimized training loop\n",
    "\n",
    "All models are a standard [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) so you can use them in any typical training loop. While you can write your own training loop, ðŸ¤— Transformers provides a [Trainer](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Trainer) class for PyTorch, which contains the basic training loop and adds additional functionality for features like distributed training, mixed precision, and more.\n",
    "\n",
    "Depending on your task, youâ€™ll typically pass the following parameters to [Trainer](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Trainer):\n",
    "\n",
    "1. Youâ€™ll start with a [PreTrainedModel](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/model#transformers.PreTrainedModel) or a [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9aaae-f36d-4dd9-ac0c-1c2edd023443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72b329-ff4f-4c63-92c0-f75d2a5a5836",
   "metadata": {},
   "source": [
    "2. [TrainingArguments](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.TrainingArguments) contains the model hyperparameters you can change like learning rate, batch size, and the number of epochs to train for. The default values are used if you donâ€™t specify any training arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5109ad-99a6-48ba-853a-db1f4fa99b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"path/to/save/folder/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ff462-7a3d-44a5-a663-2d7bc605408e",
   "metadata": {},
   "source": [
    "3. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48b620-1ba2-4f02-8c6d-6b651b547047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104beb29-84ec-4eac-b831-e21b3027704a",
   "metadata": {},
   "source": [
    "4. Load a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880db8ca-688c-41a7-ab5a-c6fc12ac33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43fdc1-94ce-4405-a344-a7c35d673901",
   "metadata": {},
   "source": [
    "5. Create a function to tokenize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd902cf7-eb43-49e9-9899-d44567b40829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279443bf-0ec0-4e8e-98e8-e74a1d1fcf16",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Then apply it over the entire dataset with map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853ae90-4c9f-4b42-8027-8cd9ba597379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777df1ed-105f-4168-8130-864ef0061f29",
   "metadata": {},
   "source": [
    "6. A [DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding) to create a batch of examples from your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7a26c-f51f-441a-82ae-a65bf6613128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db843e-3cf5-4c98-a97a-1d9dd5d640ac",
   "metadata": {},
   "source": [
    "Now gather all these classes in [Trainer](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98917125-74d1-4fc1-9cab-d68d27ca92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56b03f-da27-4ecf-841a-fae38313e796",
   "metadata": {},
   "source": [
    "When youâ€™re ready, call [train()](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Trainer.train) to start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08249beb-3b95-496b-a5f8-1c1d00862294",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4afff-c247-4273-8588-bd61b2b83338",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "\n",
    "For tasks - like translation or summarization - that use a sequence-to-sequence model, use the [Seq2SeqTrainer](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Seq2SeqTrainer) and [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments) classes instead.\n",
    "\n",
    "You can customize the training loop behavior by subclassing the methods inside [Trainer](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Trainer). This allows you to customize features such as the loss function, optimizer, and scheduler. Take a look at the [Trainer](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Trainer) reference for which methods can be subclassed.\n",
    "\n",
    "The other way to customize the training loop is by using [Callbacks](https://huggingface.co/docs/transformers/main_classes/callbacks). You can use callbacks to integrate with other libraries and inspect the training loop to report on progress or stop the training early. Callbacks do not modify anything in the training loop itself. To customize something like the loss function, you need to subclass the [Trainer](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.Trainer) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361c9cc-a85c-4076-a99b-eb6db314b752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
